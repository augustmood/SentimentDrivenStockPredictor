{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 21:12:26,691 - INFO - Initializing Spark session with optimized memory settings\n",
      "2024-04-05 21:12:26,697 - INFO - SparkSession initialization completed in 0.01 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------+\n",
      "|      date|        popularity|          positive|           neutral|          negative|ticker|\n",
      "+----------+------------------+------------------+------------------+------------------+------+\n",
      "|2012-04-11|             100.0| 31.57894736842105|21.052631578947366|47.368421052631575|  AAPL|\n",
      "|2012-04-12|              75.0|               0.0| 66.66666666666666| 33.33333333333333|  AAPL|\n",
      "|2012-04-13|             100.0|             100.0|               0.0|               0.0|  AAPL|\n",
      "|2012-04-16|             100.0|              20.0| 56.00000000000001|              24.0|  AAPL|\n",
      "|2012-04-17|             100.0|              50.0|16.666666666666664| 33.33333333333333|  AAPL|\n",
      "|2012-04-20|             100.0|               0.0| 66.66666666666666| 33.33333333333333|  AAPL|\n",
      "|2012-04-22|             100.0|               0.0|             100.0|               0.0|  AAPL|\n",
      "|2012-04-23| 23.52941176470588|              25.0|              50.0|              25.0|  AAPL|\n",
      "|2012-04-24|              95.0| 26.31578947368421| 55.26315789473685|18.421052631578945|  AAPL|\n",
      "|2012-04-25| 47.22222222222222|17.647058823529413| 52.94117647058824|29.411764705882355|  AAPL|\n",
      "|2012-04-26|21.568627450980394| 45.45454545454545| 27.27272727272727| 27.27272727272727|  AAPL|\n",
      "|2012-04-27|35.294117647058826|16.666666666666664| 33.33333333333333|              50.0|  AAPL|\n",
      "|2012-05-01|19.230769230769234|              60.0|              40.0|               0.0|  AAPL|\n",
      "|2012-05-02| 19.41747572815534|              25.0|              60.0|              15.0|  AAPL|\n",
      "|2012-05-03|              10.0| 33.33333333333333| 33.33333333333333| 33.33333333333333|  AAPL|\n",
      "|2012-05-07| 7.758620689655173| 33.33333333333333| 22.22222222222222| 44.44444444444444|  AAPL|\n",
      "|2012-05-08|3.1746031746031744| 33.33333333333333|               0.0| 66.66666666666666|  AAPL|\n",
      "|2012-05-09| 2.564102564102564|              25.0|              50.0|              25.0|  AAPL|\n",
      "|2012-05-11|10.638297872340425|13.333333333333334|              60.0|26.666666666666668|  AAPL|\n",
      "|2012-05-17| 2.898550724637681|             100.0|               0.0|               0.0|  AAPL|\n",
      "+----------+------------------+------------------+------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import yfinance as yf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, explode, when, size, avg\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "findspark.init()\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "start_time = time.time()\n",
    "def log_time_taken(start, operation):\n",
    "    end = time.time()\n",
    "    logger.info(f\"{operation} completed in {end - start:.2f} seconds\")\n",
    "\n",
    "# Start timing and log the initialization of the Spark session\n",
    "logger.info(\"Initializing Spark session with optimized memory settings\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comment Context Builder\") \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .config(\"spark.executor.memory\", \"16g\")  \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")  \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4096\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"2048\")  \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/Volumes/LaCie/wsb_archive/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"200M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.2\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\\\n",
    "    .getOrCreate()\n",
    "log_time_taken(start_time, \"SparkSession initialization\")\n",
    "\n",
    "# Define the class for simplifying company names\n",
    "class CompanyNameSimplifier:\n",
    "    def __init__(self):\n",
    "        self.suffixes = [\n",
    "            'Inc.', 'Inc', 'Corporation', 'Corp.', 'Corp', 'Company', 'Co.', 'Co', \n",
    "            'Limited', 'Ltd.', 'Ltd', 'PLC', 'NV', 'SA', 'AG', 'LLC', 'L.P.', 'LP'\n",
    "        ]\n",
    "        self.web_domains_regex = r'\\.com|\\.org|\\.net|\\.io|\\.co|\\.ai'\n",
    "\n",
    "    def simplify_company_name(self, name):\n",
    "        name = re.sub(self.web_domains_regex, '', name, flags=re.IGNORECASE)\n",
    "        for suffix in self.suffixes:\n",
    "            if name.endswith(suffix):\n",
    "                name = name.replace(suffix, '')\n",
    "                break\n",
    "        name = re.split(',| -', name)[0]\n",
    "        name = name.strip()\n",
    "        return name\n",
    "\n",
    "    def get_simplified_company_name(self, ticker):\n",
    "        company = yf.Ticker(ticker)\n",
    "        company_info = company.info\n",
    "        full_name = company_info.get('longName', '')\n",
    "        simple_name = self.simplify_company_name(full_name)\n",
    "        return simple_name\n",
    "\n",
    "class StockCommentsFilter:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.wsb_comments_with_context = spark.read.parquet(\"./wsb_comments_with_context\")\n",
    "\n",
    "    def filter_comments_by_ticker(self):\n",
    "        simplifier = CompanyNameSimplifier()\n",
    "        # Obtain the simplified company name for the given ticker\n",
    "        company_name = simplifier.get_simplified_company_name(self.ticker)\n",
    "        \n",
    "        # Convert the ticker and company name to lowercase for a case-insensitive search\n",
    "        ticker_lower = self.ticker.lower()\n",
    "        company_name_lower = company_name.lower()\n",
    "\n",
    "        # Filter the DataFrame for rows where the `comment_context` contains the ticker or the company name\n",
    "        # Uses `lower` function to ensure that the search is case-insensitive\n",
    "        filtered_df = self.wsb_comments_with_context.filter(\n",
    "            lower(col(\"comment_context\")).contains(ticker_lower) | \n",
    "            lower(col(\"comment_context\")).contains(company_name_lower)\n",
    "        ).select(\"datetime_utc\", \"comment_score\", \"comment_body\")\n",
    "        filtered_df.write.mode('overwrite').parquet(f'./stock_comments/{self.ticker}_comments')\n",
    "        return filtered_df\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.pipeline = PretrainedPipeline('analyze_sentiment', lang='en')\n",
    "        self.spark = spark\n",
    "\n",
    "    def analyze(self):\n",
    "        df = self.spark.read.parquet(f\"./stock_comments/{self.ticker}_comments\")\n",
    "        df_renamed = df.withColumnRenamed(\"comment_body\", \"text\")\n",
    "        result = self.pipeline.transform(df_renamed)\n",
    "\n",
    "        stock_sentiment = result.select(\n",
    "            col(\"datetime_utc\"),\n",
    "            col(\"comment_score\"),\n",
    "            col(\"text\").alias(\"comment_body\"),\n",
    "            col(\"sentiment.result\").alias(\"comment_sentiment\")\n",
    "        )\n",
    "\n",
    "        filtered_df = stock_sentiment.filter(size(col(\"comment_sentiment\")) > 0)\n",
    "        exploded_df = filtered_df.withColumn(\"individual_sentiment\", explode(col(\"comment_sentiment\")))\n",
    "\n",
    "        scored_df = exploded_df.withColumn(\"sentiment_score\",\n",
    "                                           when(col(\"individual_sentiment\") == \"positive\", 1)\n",
    "                                           .when(col(\"individual_sentiment\") == \"negative\", -1)\n",
    "                                           .otherwise(0))\n",
    "\n",
    "        stock_sentiment = scored_df.groupBy(\"datetime_utc\", \"comment_score\", \"comment_body\").agg(avg(\"sentiment_score\").alias(\"sentiment_score\"))\n",
    "        stock_sentiment = stock_sentiment.orderBy(\"datetime_utc\")\n",
    "        stock_sentiment.write.mode('overwrite').parquet(f\"./stock_sentiments/{self.ticker}_sentiment\")\n",
    "        return stock_sentiment\n",
    "\n",
    "class PopularityCalculator:\n",
    "    def __init__(self, ticker, df, simplifier):\n",
    "        self.ticker = ticker\n",
    "        self.df = df\n",
    "        self.simplifier = simplifier\n",
    "\n",
    "    def calculate_popularity(self):\n",
    "        # Convert to Eastern Time and simplify the company name\n",
    "        df = self.df.withColumn(\"datetime_et\", F.expr(\"from_utc_timestamp(datetime_utc, 'America/New_York')\"))\n",
    "        simplified_name = self.simplifier.get_simplified_company_name(self.ticker).lower()\n",
    "\n",
    "        # Filter comments by ticker or company name\n",
    "        filtered_comments = df.filter(\n",
    "            lower(col(\"comment_context\")).contains(self.ticker.lower()) |\n",
    "            lower(col(\"comment_context\")).contains(simplified_name)\n",
    "        )\n",
    "\n",
    "        # Aggregate daily mentions and total comments\n",
    "        ticker_mentions = filtered_comments.groupBy(F.to_date(\"datetime_et\").alias(\"date\")).count().withColumnRenamed(\"count\", \"ticker_mentions\")\n",
    "        total_comments = df.groupBy(F.to_date(\"datetime_et\").alias(\"date\")).count().withColumnRenamed(\"count\", \"total_comments\")\n",
    "\n",
    "        # Calculate popularity percentage and sort by date\n",
    "        popularity = ticker_mentions.join(total_comments, on=\"date\") \\\n",
    "            .withColumn(\"popularity_percentage\", F.col(\"ticker_mentions\") / F.col(\"total_comments\") * 100) \\\n",
    "            .orderBy(\"date\")\n",
    "\n",
    "        # Save the result\n",
    "        save_path = f'./stock_popularity/{self.ticker}_popularity'\n",
    "        popularity.write.mode('overwrite').parquet(save_path)\n",
    "\n",
    "        return popularity\n",
    "\n",
    "\n",
    "class StockSentimentPercentageAnalyzer:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.df = spark.read.parquet(f'./stock_sentiments/{ticker}_sentiment')\n",
    "\n",
    "    def categorize_sentiment(self):\n",
    "        df_with_sentiment_category = self.df.withColumn(\n",
    "            \"sentiment_category\",\n",
    "            when(self.df.sentiment_score > 0.05, \"positive\")\n",
    "            .when(self.df.sentiment_score < -0.05, \"negative\")\n",
    "            .otherwise(\"neutral\")\n",
    "        )\n",
    "        return df_with_sentiment_category\n",
    "\n",
    "    def analyze_sentiment(self):\n",
    "        df = self.categorize_sentiment()\n",
    "        df = df.withColumn(\"datetime_et\", F.expr(\"from_utc_timestamp(datetime_utc, 'America/New_York')\"))\n",
    "        df = df.withColumn(\"date\", F.to_date(\"datetime_et\"))\n",
    "\n",
    "        result = df.groupBy(\"date\").agg(\n",
    "            F.expr(\"count(1) as total_mentions\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_category\") == \"positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_category\") == \"neutral\", 1).otherwise(0)).alias(\"neutral_count\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_category\") == \"negative\", 1).otherwise(0)).alias(\"negative_count\")\n",
    "        ).withColumn(\n",
    "            \"positive_percentage\", F.col(\"positive_count\") / F.col(\"total_mentions\") * 100\n",
    "        ).withColumn(\n",
    "            \"neutral_percentage\", F.col(\"neutral_count\") / F.col(\"total_mentions\") * 100\n",
    "        ).withColumn(\n",
    "            \"negative_percentage\", F.col(\"negative_count\") / F.col(\"total_mentions\") * 100\n",
    "        )\n",
    "\n",
    "        result = result.orderBy(\"date\")\n",
    "        result.write.mode('overwrite').parquet(f\"./stock_sentiments_percentage/{self.ticker}_sentiment_percentage\")\n",
    "        return result\n",
    "\n",
    "class StockDataMerger:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.spark = spark\n",
    "\n",
    "    def merge_data(self):\n",
    "        # Read stock popularity and sentiment percentage data\n",
    "        stock_popularity = self.spark.read.parquet(f\"./stock_popularity/{self.ticker}_popularity\")\n",
    "        stock_sentiment_percentage = self.spark.read.parquet(f\"./stock_sentiments_percentage/{self.ticker}_sentiment_percentage\")\n",
    "\n",
    "        # Inner join on date\n",
    "        stock_sentiment_and_popularity = stock_popularity.join(stock_sentiment_percentage, \"date\", \"inner\")\n",
    "\n",
    "        # Selecting and renaming the desired columns\n",
    "        stock_sentiment_and_popularity = stock_sentiment_and_popularity.select(\n",
    "            col(\"date\"),\n",
    "            col(\"total_mentions\").alias(\"mentions\"),\n",
    "            col(\"popularity_percentage\").alias(\"popularity\"),\n",
    "            col(\"positive_percentage\").alias(\"positive\"),\n",
    "            col(\"neutral_percentage\").alias(\"neutral\"),\n",
    "            col(\"negative_percentage\").alias(\"negative\")\n",
    "        )\n",
    "        # Add a new column with the ticker\n",
    "        stock_sentiment_and_popularity = stock_sentiment_and_popularity.withColumn('ticker', F.lit(self.ticker))\n",
    "\n",
    "        stock_sentiment_and_popularity.write.mode('overwrite').parquet(f\"./stock_sentiment_and_popularity/{self.ticker}_sentiment_and_popularity\")\n",
    "        return stock_sentiment_and_popularity\n",
    "\n",
    "def main():\n",
    "    # Step 01: Filter comments by ticker\n",
    "    ticker = 'AAPL'\n",
    "    stock_filter = StockCommentsFilter(ticker)\n",
    "    stock_filter.filter_comments_by_ticker()\n",
    "\n",
    "    # Step 02: Analyze sentiment\n",
    "    analyzer = SentimentAnalyzer(ticker)\n",
    "    analyzer.analyze()\n",
    "\n",
    "    # Step 03: Calculate popularity\n",
    "    df = spark.read.parquet(\"./wsb_comments_with_context\")\n",
    "    simplifier = CompanyNameSimplifier()\n",
    "    popularity_calculator = PopularityCalculator(ticker, df, simplifier)\n",
    "    popularity_calculator.calculate_popularity()\n",
    "\n",
    "    # Step 04: Calculate sentiment percentage\n",
    "    analyzer = StockSentimentPercentageAnalyzer(ticker)\n",
    "    analyzer.analyze_sentiment()\n",
    "\n",
    "    # Step 05: Merge popularity and sentiment percentage data\n",
    "    merger = StockDataMerger(ticker)\n",
    "    merger.merge_data()\n",
    "    \n",
    "    # Step 06: Show the merged data\n",
    "    spark.read.parquet(f\"./stock_sentiment_and_popularity/{ticker}_sentiment_and_popularity\").show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+\n",
      "|       datetime_utc|comment_score|        comment_body|\n",
      "+-------------------+-------------+--------------------+\n",
      "|2022-12-21 12:25:36|           -2|Never said he had...|\n",
      "|2022-12-21 12:25:56|            1|Jim Cramer says s...|\n",
      "|2022-12-21 12:26:05|           -5|TSLA ![img](emote...|\n",
      "|2022-12-21 12:26:16|            1|me neither. He is...|\n",
      "|2022-12-21 12:26:24|            1|The data is comin...|\n",
      "|2022-12-21 12:27:05|            2|          Not really|\n",
      "|2022-12-21 12:27:15|            1|What percentage o...|\n",
      "|2022-12-21 12:27:16|            1|Trump 2.0. Elon g...|\n",
      "|2022-12-21 12:27:28|            2|             Really?|\n",
      "|2022-12-21 12:27:34|            5|        Flat as fuck|\n",
      "|2022-12-21 12:28:07|            2|According to my b...|\n",
      "|2022-12-21 12:28:31|            2|himm .. that soun...|\n",
      "|2022-12-21 12:28:39|            1|Fuck off you litt...|\n",
      "|2022-12-21 12:28:50|            4|More reason for a...|\n",
      "|2022-12-21 12:29:04|            1|           [removed]|\n",
      "|2022-12-21 12:29:52|            1|At this point, wh...|\n",
      "|2022-12-21 12:29:55|            5|TESLA PIECE OF SH...|\n",
      "|2022-12-21 12:29:58|            6|Come on TSLA mah ...|\n",
      "|2022-12-21 12:30:13|            4|*Note: 498 of tho...|\n",
      "|2022-12-21 12:30:23|            3|The problem with ...|\n",
      "+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = 'TSLA'\n",
    "spark.read.parquet(f'./stock_sentiments/{ticker}_sentiment').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
