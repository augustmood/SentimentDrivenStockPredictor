{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 06:54:40,833 - INFO - Initializing Spark session with optimized memory settings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 06:54:42 WARN Utils: Your hostname, Binmings-iMac-5.local resolves to a loopback address: 127.0.0.1; using 192.168.1.69 instead (on interface en1)\n",
      "24/04/08 06:54:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/binmingli/spark-3.3.3-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/binmingli/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/binmingli/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8aa2b225-1132-4cab-97fc-a11417394d0b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.17.0 in central\n",
      ":: resolution report :: resolve 800ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.17.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   83  |   0   |   0   |   5   ||   78  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8aa2b225-1132-4cab-97fc-a11417394d0b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 78 already retrieved (0kB/12ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 06:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 06:54:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/08 06:54:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/04/08 06:54:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 06:54:48,042 - INFO - SparkSession initialization completed in 7.21 seconds\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[datetime_utc: string, comment_id: string, submission_id: string, parent_id: string, comment_score: double, comment_body: string, curr_parent_id: string, comment_context: string, reached_top: boolean]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, concat\n",
    "import findspark\n",
    "import logging\n",
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, concat_ws, lit, expr, greatest\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_time_taken(start, operation):\n",
    "    end = time.time()\n",
    "    logger.info(f\"{operation} completed in {end - start:.2f} seconds\")\n",
    "\n",
    "# Start timing and log the initialization of the Spark session\n",
    "logger.info(\"Initializing Spark session with optimized memory settings\")\n",
    "start_time = time.time()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comment Context Builder\") \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .config(\"spark.executor.memory\", \"64g\")  \\\n",
    "    .config(\"spark.driver.memory\", \"32g\")  \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4096\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"2048\")  \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/Volumes/LaCie/wsb_archive/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.2\")\\\n",
    "    .getOrCreate()\n",
    "log_time_taken(start_time, \"SparkSession initialization\")\n",
    "\n",
    "\n",
    "def process_files(csv_folder, column_selection, column_rename, output_file):\n",
    "    \"\"\"\n",
    "    Processes CSV files from a specified folder into a single DataFrame, \n",
    "    selects specific columns, renames them, and writes to a parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_folder: Folder path containing CSV files.\n",
    "    - column_selection: List of columns to select from the DataFrame.\n",
    "    - column_rename: Dictionary mapping original column names to new names.\n",
    "    - output_file: Path to the output parquet file.\n",
    "    \"\"\"\n",
    "    # Generate file path pattern\n",
    "    file_pattern = f'{csv_folder}/*.csv'\n",
    "    \n",
    "    # Get a list of all CSV files in the specified folder\n",
    "    csv_files = glob.glob(file_pattern)\n",
    "\n",
    "    # Read CSV files and append to a list of DataFrames\n",
    "    dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Select and rename specified columns\n",
    "    combined_df = combined_df[column_selection].rename(columns=column_rename)\n",
    "    \n",
    "    # Write to a parquet file\n",
    "    combined_df.to_parquet(output_file)\n",
    "\n",
    "def build_context_chain(comments: DataFrame, submissions: DataFrame, max_depth: int = None) -> DataFrame:\n",
    "    # Alias submissions and comments with unique column names\n",
    "    submissions_kv = submissions.select(\n",
    "        expr(\"submission_id as s_key\"),\n",
    "        concat_ws(\" \", col(\"title\"), col(\"self_text\")).alias(\"s_value\")\n",
    "    )\n",
    "\n",
    "    # Including submission_id in the comments_kv DataFrame and introducing the reached_top flag\n",
    "    comments_kv = comments.select(\n",
    "        expr(\"comment_id as c_key\"),\n",
    "        col(\"parent_id\").alias(\"c_parent_id\"),\n",
    "        col(\"comment_body\").alias(\"c_value\"),\n",
    "        lit(False).alias(\"reached_top\"),  # Initial reached_top flag set to False\n",
    "    ).withColumn(\"curr_parent_id\", col(\"c_parent_id\"))  # Initialize curr_parent_id\n",
    "\n",
    "    # Initialize context with the comment itself\n",
    "    context_df = comments_kv.withColumn(\"context\", col(\"c_value\"))\n",
    "\n",
    "    # Repartition DataFrames to optimize join performance\n",
    "    submissions_kv = submissions_kv.repartition(200)\n",
    "    comments_kv = comments_kv.repartition(200)\n",
    "    context_df = context_df.repartition(200)\n",
    "    i = 1\n",
    "    while True and (max_depth is None or i <= max_depth):\n",
    "        comments_iter = comments_kv.alias(f\"c{i}\")\n",
    "        \n",
    "        context_df = context_df.join(\n",
    "            comments_iter,\n",
    "            context_df[\"curr_parent_id\"] == expr(f\"concat('t1_', c{i}.c_key)\"),\n",
    "            \"left_outer\"\n",
    "        ).join(\n",
    "            submissions_kv,\n",
    "            context_df[\"curr_parent_id\"] == expr(f\"concat('t3_', s_key)\"),\n",
    "            \"left_outer\"\n",
    "        ).select(\n",
    "            context_df[\"c_key\"],\n",
    "            when(\n",
    "                context_df[\"curr_parent_id\"].startswith(\"t3_\"), \n",
    "                concat_ws(\" |->| \", context_df[\"context\"], submissions_kv[\"s_value\"])\n",
    "            ).when(\n",
    "               col(f\"c{i}.c_parent_id\").isNull(),\n",
    "                concat_ws(\" |->| \", context_df[\"context\"], lit(\"...\"))\n",
    "            ).when(\n",
    "                context_df[\"curr_parent_id\"].startswith(\"t1_\"), \n",
    "                concat_ws(\" |->| \", context_df[\"context\"], col(f\"c{i}.c_value\"))\n",
    "            ).otherwise(context_df[\"context\"]).alias(\"context\"),\n",
    "            # Update curr_parent_id based on the join result\n",
    "            when(context_df[\"curr_parent_id\"].startswith(\"t1_\"), col(f\"c{i}.c_parent_id\")).otherwise(context_df[\"curr_parent_id\"]).alias(\"curr_parent_id\"),\n",
    "            # Update reached_top flag\n",
    "            when(context_df[\"curr_parent_id\"].isNull(), lit(True)).\n",
    "            when(context_df[\"curr_parent_id\"].startswith(\"t3_\"), lit(True))\n",
    "            .when(col(f\"c{i}.curr_parent_id\").isNull(), lit(True))\n",
    "            .otherwise(context_df[\"reached_top\"])\n",
    "            .alias(\"reached_top\")\n",
    "        )\n",
    "        # Check if all rows have reached the top; if so, break the loop\n",
    "        if context_df.filter(col(\"reached_top\") == False).count() == 0:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "    # Final join with original comments DataFrame to include additional details\n",
    "    final_df = comments.join(\n",
    "        context_df,\n",
    "        comments[\"comment_id\"] == context_df[\"c_key\"],\n",
    "        \"left_outer\"\n",
    "    ).select(\n",
    "        comments[\"datetime_utc\"], comments[\"comment_id\"], comments[\"submission_id\"], \n",
    "        comments[\"parent_id\"], comments[\"comment_score\"], comments[\"comment_body\"], \n",
    "        context_df[\"curr_parent_id\"],\n",
    "        context_df[\"context\"].alias(\"comment_context\"), context_df[\"reached_top\"]\n",
    "    )\n",
    "\n",
    "    final_df = final_df.dropna(subset=[\"datetime_utc\"]).dropDuplicates(['comment_id'])\n",
    "    final_df = final_df.orderBy(\"datetime_utc\", ascending=True)\n",
    "    final_df.write.mode(\"overwrite\").parquet(\"./sentiment_temp/wsb_comments_with_context\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Define the column selection and renaming for comments\n",
    "comments_columns = ['Datetime', 'Body', 'ID', 'Parent ID', 'Submission ID', 'Score']\n",
    "comments_rename = {\n",
    "    'Datetime': 'datetime_utc',\n",
    "    'Body': 'comment_body',\n",
    "    'ID': 'comment_id',\n",
    "    'Parent ID': 'parent_id',\n",
    "    'Submission ID': 'submission_id',\n",
    "    'Score': 'comment_score'\n",
    "}\n",
    "\n",
    "# Define the column selection and renaming for submissions\n",
    "submissions_columns = ['Datetime', 'Title', 'Body', 'ID', 'Score']\n",
    "submissions_rename = {\n",
    "    'Datetime': 'datetime_utc',\n",
    "    'Title': 'title',\n",
    "    'Body': 'self_text',\n",
    "    'ID': 'submission_id',\n",
    "    'Score': 'submission_score'\n",
    "}\n",
    "\n",
    "if not os.path.exists('temp'):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs('temp')\n",
    "if not os.path.exists('sentiment_temp'):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs('sentiment_temp')\n",
    "# Process comments CSV files\n",
    "process_files('./input_data/wsb-comments', comments_columns, comments_rename, './temp/new_comments.parquet')\n",
    "# Process submissions CSV files\n",
    "process_files('./input_data/wsb-submissions', submissions_columns, submissions_rename, './temp/new_submissions.parquet')\n",
    "new_comments = spark.read.parquet(\"./temp/new_comments.parquet\")\n",
    "new_submissions = spark.read.parquet(\"./temp/new_submissions.parquet\")\n",
    "build_context_chain(new_comments, new_submissions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import yfinance as yf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, explode, when, size, avg\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "findspark.init()\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "start_time = time.time()\n",
    "def log_time_taken(start, operation):\n",
    "    end = time.time()\n",
    "    logger.info(f\"{operation} completed in {end - start:.2f} seconds\")\n",
    "\n",
    "# Start timing and log the initialization of the Spark session\n",
    "logger.info(\"Initializing Spark session with optimized memory settings\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comment Context Builder\") \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .config(\"spark.executor.memory\", \"16g\")  \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")  \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4096\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"2048\")  \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/Volumes/LaCie/wsb_archive/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"200M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.2\")\\\n",
    "    .getOrCreate()\n",
    "log_time_taken(start_time, \"SparkSession initialization\")\n",
    "\n",
    "# Define the class for simplifying company names\n",
    "class CompanyNameSimplifier:\n",
    "    def __init__(self):\n",
    "        self.suffixes = [\n",
    "            'Inc.', 'Inc', 'Corporation', 'Corp.', 'Corp', 'Company', 'Co.', 'Co', \n",
    "            'Limited', 'Ltd.', 'Ltd', 'PLC', 'NV', 'SA', 'AG', 'LLC', 'L.P.', 'LP'\n",
    "        ]\n",
    "        self.web_domains_regex = r'\\.com|\\.org|\\.net|\\.io|\\.co|\\.ai'\n",
    "\n",
    "    def simplify_company_name(self, name):\n",
    "        name = re.sub(self.web_domains_regex, '', name, flags=re.IGNORECASE)\n",
    "        for suffix in self.suffixes:\n",
    "            if name.endswith(suffix):\n",
    "                name = name.replace(suffix, '')\n",
    "                break\n",
    "        name = re.split(',| -', name)[0]\n",
    "        name = name.strip()\n",
    "        return name\n",
    "\n",
    "    def get_simplified_company_name(self, ticker):\n",
    "        company = yf.Ticker(ticker)\n",
    "        company_info = company.info\n",
    "        full_name = company_info.get('longName', '')\n",
    "        simple_name = self.simplify_company_name(full_name)\n",
    "        return simple_name\n",
    "\n",
    "class StockCommentsFilter:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.wsb_comments_with_context = spark.read.parquet(\"./sentiment_temp/wsb_comments_with_context\")\n",
    "\n",
    "    def filter_comments_by_ticker(self):\n",
    "        simplifier = CompanyNameSimplifier()\n",
    "        # Obtain the simplified company name for the given ticker\n",
    "        company_name = simplifier.get_simplified_company_name(self.ticker)\n",
    "        \n",
    "        # Convert the ticker and company name to lowercase for a case-insensitive search\n",
    "        ticker_lower = self.ticker.lower()\n",
    "        company_name_lower = company_name.lower()\n",
    "\n",
    "        # Filter the DataFrame for rows where the `comment_context` contains the ticker or the company name\n",
    "        # Uses `lower` function to ensure that the search is case-insensitive\n",
    "        filtered_df = self.wsb_comments_with_context.filter(\n",
    "            lower(col(\"comment_context\")).contains(ticker_lower) | \n",
    "            lower(col(\"comment_context\")).contains(company_name_lower)\n",
    "        ).select(\"datetime_utc\", \"comment_score\", \"comment_body\")\n",
    "        filtered_df.write.mode('overwrite').parquet(f'./sentiment_temp/stock_comments/{self.ticker}_comments')\n",
    "        return filtered_df\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.pipeline = PretrainedPipeline('analyze_sentiment', lang='en')\n",
    "        self.spark = spark\n",
    "\n",
    "    def analyze(self):\n",
    "        df = self.spark.read.parquet(f\"./sentiment_temp/stock_comments/{self.ticker}_comments\")\n",
    "        df_renamed = df.withColumnRenamed(\"comment_body\", \"text\")\n",
    "        result = self.pipeline.transform(df_renamed)\n",
    "\n",
    "        stock_sentiment = result.select(\n",
    "            col(\"datetime_utc\"),\n",
    "            col(\"comment_score\"),\n",
    "            col(\"text\").alias(\"comment_body\"),\n",
    "            col(\"sentiment.result\").alias(\"comment_sentiment\")\n",
    "        )\n",
    "\n",
    "        filtered_df = stock_sentiment.filter(size(col(\"comment_sentiment\")) > 0)\n",
    "        exploded_df = filtered_df.withColumn(\"individual_sentiment\", explode(col(\"comment_sentiment\")))\n",
    "\n",
    "        scored_df = exploded_df.withColumn(\"sentiment_score\",\n",
    "                                           when(col(\"individual_sentiment\") == \"positive\", 1)\n",
    "                                           .when(col(\"individual_sentiment\") == \"negative\", -1)\n",
    "                                           .otherwise(0))\n",
    "\n",
    "        stock_sentiment = scored_df.groupBy(\"datetime_utc\", \"comment_score\", \"comment_body\").agg(avg(\"sentiment_score\").alias(\"sentiment_score\"))\n",
    "        stock_sentiment = stock_sentiment.orderBy(\"datetime_utc\")\n",
    "        stock_sentiment.write.mode('overwrite').parquet(f\"./sentiment_temp/stock_sentiments/{self.ticker}_sentiment\")\n",
    "        return stock_sentiment\n",
    "\n",
    "class PopularityCalculator:\n",
    "    def __init__(self, ticker, df, simplifier):\n",
    "        self.ticker = ticker\n",
    "        self.df = df\n",
    "        self.simplifier = simplifier\n",
    "\n",
    "    def calculate_popularity(self):\n",
    "        # Convert to Eastern Time and simplify the company name\n",
    "        df = self.df.withColumn(\"datetime_et\", F.expr(\"from_utc_timestamp(datetime_utc, 'America/New_York')\"))\n",
    "        simplified_name = self.simplifier.get_simplified_company_name(self.ticker).lower()\n",
    "\n",
    "        # Filter comments by ticker or company name\n",
    "        filtered_comments = df.filter(\n",
    "            lower(col(\"comment_context\")).contains(self.ticker.lower()) |\n",
    "            lower(col(\"comment_context\")).contains(simplified_name)\n",
    "        )\n",
    "\n",
    "        # Aggregate daily mentions and total comments\n",
    "        ticker_mentions = filtered_comments.groupBy(F.to_date(\"datetime_et\").alias(\"date\")).count().withColumnRenamed(\"count\", \"ticker_mentions\")\n",
    "        total_comments = df.groupBy(F.to_date(\"datetime_et\").alias(\"date\")).count().withColumnRenamed(\"count\", \"total_comments\")\n",
    "\n",
    "        # Calculate popularity percentage and sort by date\n",
    "        popularity = ticker_mentions.join(total_comments, on=\"date\") \\\n",
    "            .withColumn(\"popularity_percentage\", F.col(\"ticker_mentions\") / F.col(\"total_comments\") * 100) \\\n",
    "            .orderBy(\"date\")\n",
    "\n",
    "        # Save the result\n",
    "        save_path = f'./sentiment_temp/stock_popularity/{self.ticker}_popularity'\n",
    "        popularity.write.mode('overwrite').parquet(save_path)\n",
    "\n",
    "        return popularity\n",
    "\n",
    "\n",
    "class StockSentimentPercentageAnalyzer:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.df = spark.read.parquet(f'./sentiment_temp/stock_sentiments/{ticker}_sentiment')\n",
    "\n",
    "    def categorize_sentiment(self):\n",
    "        df_with_sentiment_category = self.df.withColumn(\n",
    "            \"sentiment_category\",\n",
    "            when(self.df.sentiment_score > 0.05, \"positive\")\n",
    "            .when(self.df.sentiment_score < -0.05, \"negative\")\n",
    "            .otherwise(\"neutral\")\n",
    "        )\n",
    "        return df_with_sentiment_category\n",
    "\n",
    "    def analyze_sentiment(self):\n",
    "        df = self.categorize_sentiment()\n",
    "        df = df.withColumn(\"datetime_et\", F.expr(\"from_utc_timestamp(datetime_utc, 'America/New_York')\"))\n",
    "        df = df.withColumn(\"date\", F.to_date(\"datetime_et\"))\n",
    "\n",
    "        result = df.groupBy(\"date\").agg(\n",
    "            F.expr(\"count(1) as total_mentions\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_category\") == \"positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_category\") == \"neutral\", 1).otherwise(0)).alias(\"neutral_count\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_category\") == \"negative\", 1).otherwise(0)).alias(\"negative_count\")\n",
    "        ).withColumn(\n",
    "            \"positive_percentage\", F.col(\"positive_count\") / F.col(\"total_mentions\") * 100\n",
    "        ).withColumn(\n",
    "            \"neutral_percentage\", F.col(\"neutral_count\") / F.col(\"total_mentions\") * 100\n",
    "        ).withColumn(\n",
    "            \"negative_percentage\", F.col(\"negative_count\") / F.col(\"total_mentions\") * 100\n",
    "        )\n",
    "\n",
    "        result = result.orderBy(\"date\")\n",
    "        result.write.mode('overwrite').parquet(f\"./sentiment_temp/stock_sentiments_percentage/{self.ticker}_sentiment_percentage\")\n",
    "        return result\n",
    "\n",
    "class StockDataMerger:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "        self.spark = spark\n",
    "\n",
    "    def merge_data(self):\n",
    "        # Read stock popularity and sentiment percentage data\n",
    "        stock_popularity = self.spark.read.parquet(f\"./sentiment_temp/stock_popularity/{self.ticker}_popularity\")\n",
    "        stock_sentiment_percentage = self.spark.read.parquet(f\"./sentiment_temp/stock_sentiments_percentage/{self.ticker}_sentiment_percentage\")\n",
    "\n",
    "        # Inner join on date\n",
    "        stock_sentiment_and_popularity = stock_popularity.join(stock_sentiment_percentage, \"date\", \"inner\")\n",
    "\n",
    "        # Selecting and renaming the desired columns\n",
    "        stock_sentiment_and_popularity = stock_sentiment_and_popularity.select(\n",
    "            col(\"date\"),\n",
    "            col(\"total_mentions\").alias(\"mentions\"),\n",
    "            col(\"popularity_percentage\").alias(\"popularity\"),\n",
    "            col(\"positive_percentage\").alias(\"positive\"),\n",
    "            col(\"neutral_percentage\").alias(\"neutral\"),\n",
    "            col(\"negative_percentage\").alias(\"negative\")\n",
    "        )\n",
    "        # Add a new column with the ticker\n",
    "        stock_sentiment_and_popularity = stock_sentiment_and_popularity.withColumn('ticker', F.lit(self.ticker))\n",
    "\n",
    "        stock_sentiment_and_popularity.write.mode('overwrite').parquet(f\"./temp/stock_sentiment_and_popularity/{self.ticker}_sentiment_and_popularity\")\n",
    "        return stock_sentiment_and_popularity\n",
    "\n",
    "def run_pipeline(ticker):\n",
    "    # Step 01: Filter comments by ticker\n",
    "    stock_filter = StockCommentsFilter(ticker)\n",
    "    stock_filter.filter_comments_by_ticker()\n",
    "\n",
    "    # # Step 02: Analyze sentiment\n",
    "    analyzer = SentimentAnalyzer(ticker)\n",
    "    analyzer.analyze()\n",
    "\n",
    "    # # Step 03: Calculate popularity\n",
    "    df = spark.read.parquet(\"./sentiment_temp/wsb_comments_with_context\")\n",
    "    simplifier = CompanyNameSimplifier()\n",
    "    popularity_calculator = PopularityCalculator(ticker, df, simplifier)\n",
    "    popularity_calculator.calculate_popularity()\n",
    "\n",
    "    # Step 04: Calculate sentiment percentage\n",
    "    analyzer = StockSentimentPercentageAnalyzer(ticker)\n",
    "    analyzer.analyze_sentiment()\n",
    "\n",
    "    # Step 05: Merge popularity and sentiment percentage data\n",
    "    merger = StockDataMerger(ticker)\n",
    "    merger.merge_data()\n",
    "    \n",
    "    # Step 06: Show the merged data\n",
    "    spark.read.parquet(f\"./temp/stock_sentiment_and_popularity/{ticker}_sentiment_and_popularity\").show()\n",
    "\n",
    "def main():\n",
    "    tickers = [\"AAPL\", \"NVDA\", \"TSLA\"]\n",
    "    for ticker in tickers:\n",
    "        run_pipeline(ticker)\n",
    "    dir_path = \"./sentiment_temp\"\n",
    "    if os.path.exists(dir_path):\n",
    "        # Recursively delete the directory\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(f\"The directory '{dir_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"The directory '{dir_path}' does not exist.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_weighted_avg = df_cp.groupby('date').apply(\n",
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_weighted_avg = df_cp.groupby('date').apply(\n",
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_weighted_avg = df_cp.groupby('date').apply(\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:117: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_stock_data = combined_stock_data.groupby('ticker', group_keys=False).apply(lambda group: group.fillna(method='ffill'))\n",
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:117: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined_stock_data = combined_stock_data.groupby('ticker', group_keys=False).apply(lambda group: group.fillna(method='ffill'))\n",
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:118: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_stock_data = combined_stock_data.groupby('ticker', group_keys=False).apply(lambda group: group.fillna(method='bfill'))\n",
      "/var/folders/tc/l35k9zj551s7zrx9_y666c_00000gn/T/ipykernel_22666/4142067740.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined_stock_data = combined_stock_data.groupby('ticker', group_keys=False).apply(lambda group: group.fillna(method='bfill'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def read_and_process_parquet(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df = df.drop_duplicates(subset=['title', 'time_published', 'url'])\n",
    "    df['date'] = pd.to_datetime(df['time_published']).dt.strftime('%Y-%m-%d')\n",
    "    return df\n",
    "\n",
    "def compute_priority_and_weighted_score(df: pd.DataFrame, priority_score: dict) -> pd.DataFrame:\n",
    "    df_cp = df.copy()\n",
    "    df_cp['priority'] = df_cp['source'].map(priority_score)\n",
    "    df_cp['weighted_score'] = df_cp.overall_sentiment_score * df_cp.priority\n",
    "    return df_cp\n",
    "\n",
    "def calculate_daily_weighted_avg(df_cp: pd.DataFrame, company_name: str) -> pd.DataFrame:\n",
    "    daily_weighted_avg = df_cp.groupby('date').apply(\n",
    "        lambda x: (x['weighted_score'].sum() / x['priority'].sum()) if x['priority'].sum() != 0 else 0\n",
    "    ).reset_index(name='daily_weighted_avg')\n",
    "    daily_weighted_avg['ticker'] = company_name\n",
    "    return daily_weighted_avg\n",
    "\n",
    "def combine_daily_averages(*dfs) -> pd.DataFrame:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], format='%Y-%m-%d')\n",
    "    return combined_df\n",
    "\n",
    "def process_ticker_data(ticker: str, priority_score: dict) -> pd.DataFrame:\n",
    "    df = read_and_process_parquet(f'./input_data/stock-news/{ticker}_news.parquet')\n",
    "    df_cp = compute_priority_and_weighted_score(df, priority_score)\n",
    "    daily_weighted_avg = calculate_daily_weighted_avg(df_cp, ticker)\n",
    "    return daily_weighted_avg\n",
    "\n",
    "def get_combined_stock_price_data(tickers):\n",
    "    # Calculate dates for the past year\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=500)\n",
    "\n",
    "    # Format dates in YYYY-MM-DD format\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # DataFrame to store combined stock data\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # Fetch stock data\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "        # Add a 'Ticker' column\n",
    "        data['ticker'] = ticker\n",
    "        # Reset the index to make 'Date' a column\n",
    "        data = data.reset_index()\n",
    "        # Rename 'Date' column to 'date'\n",
    "        data = data.rename(columns={'Date': 'date'})\n",
    "        # Append to the combined DataFrame\n",
    "        combined_data = pd.concat([combined_data, data])\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def main():\n",
    "    # Priority score dictionary is defined only once\n",
    "    priority_score = {\n",
    "        'The Week News': 1,\n",
    "        'Wall Street Journal': 4,\n",
    "        'GlobeNewswire': 1,\n",
    "        'Zacks Commentary': 1,\n",
    "        'Reuters': 4,\n",
    "        'CNBC': 1,\n",
    "        'The Atlantic': 1,\n",
    "        'New York Times': 4.5,\n",
    "        'Decrypt.co': 1,\n",
    "        'Investing News Network': 1,\n",
    "        'Investors Business Daily': 1,\n",
    "        'The Block Crypto': 1,\n",
    "        'StockMarket.com': 1,\n",
    "        'Forbes': 3.5,\n",
    "        'Fox Business News': 1,\n",
    "        'The Financial Express': 1,\n",
    "        'Motley Fool': 1,\n",
    "        'Cointelegraph': 1,\n",
    "        'PennyStocks.com': 1,\n",
    "        'The Street': 1,\n",
    "        'Economic Times': 1,\n",
    "        'Money Control': 1,\n",
    "        'Al Jareeza': 1,\n",
    "        'Benzinga': 1,\n",
    "        'Axios': 1,\n",
    "        'MarketWatch': 1,\n",
    "        'CNN': 4,\n",
    "        'Stocknews.com': 1,\n",
    "        'The Economist': 4,\n",
    "        'Money Morning': 1,\n",
    "        'Kiplinger': 1,\n",
    "        'Associated Press': 4.5,\n",
    "        'Barrons': 1,\n",
    "        'Financial News London': 1,\n",
    "        'FinancialBuzz': 1,\n",
    "        'Fast Company': 1,\n",
    "        'Financial Times': 4,\n",
    "        'Business Standard': 1,\n",
    "        'UPI Business': 1,\n",
    "        'South China Morning Post': 3.5,\n",
    "        'Business Insider': 1,\n",
    "        'Investor Ideas': 1,\n",
    "        'Canada Newswire': 1,\n",
    "        'PR Newswire': 1\n",
    "    }\n",
    "    \n",
    "    tickers = ['AAPL', 'NVDA', 'TSLA']\n",
    "    daily_weighted_avgs = [process_ticker_data(ticker, priority_score) for ticker in tickers]\n",
    "    \n",
    "    combined_daily_averages = combine_daily_averages(*daily_weighted_avgs)\n",
    "    combined_stock_price = get_combined_stock_price_data(tickers)\n",
    "    combined_stock_data = pd.merge(combined_stock_price, combined_daily_averages, on=['date', 'ticker'], how='left')\n",
    "    combined_stock_data = combined_stock_data.groupby('ticker', group_keys=False).apply(lambda group: group.fillna(method='ffill'))\n",
    "    combined_stock_data = combined_stock_data.groupby('ticker', group_keys=False).apply(lambda group: group.fillna(method='bfill'))  \n",
    "    combined_stock_data.to_csv('./temp/stock_news_combined.csv', index=False)\n",
    "    return combined_stock_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class StockDataProcessor:\n",
    "    def __init__(self, stock_data_file):\n",
    "        self.stock_data_file = stock_data_file\n",
    "        self.df = None\n",
    "        self.combined_df = None\n",
    "\n",
    "    def load_stock_data(self):\n",
    "        self.df = pd.read_csv(self.stock_data_file)\n",
    "        self.df = self.df.rename(columns={'Date': 'date', 'company_name': 'ticker'})\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'], format='%Y-%m-%d')\n",
    "\n",
    "    def generate_sentiment_file_names(self, tickers):\n",
    "        \"\"\"Generates file paths for sentiment data based on ticker symbols.\"\"\"\n",
    "        return [f'./temp/stock_sentiment_and_popularity/{ticker}_sentiment_and_popularity' for ticker in tickers]\n",
    "\n",
    "    def load_and_combine_sentiment_data(self, tickers):\n",
    "        files = self.generate_sentiment_file_names(tickers)\n",
    "        dfs = []\n",
    "        for file in files:\n",
    "            temp_df = pd.read_parquet(file)\n",
    "            dfs.append(temp_df)\n",
    "\n",
    "        self.combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        self.combined_df['date'] = pd.to_datetime(self.combined_df['date'], format='%Y-%m-%d')\n",
    "\n",
    "    def merge_dataframes(self):\n",
    "        self.combined_df = pd.merge(self.df, self.combined_df, on=['date', 'ticker'], how='left')\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    stock_data_processor = StockDataProcessor('./temp/stock_news_combined.csv')\n",
    "    stock_data_processor.load_stock_data()\n",
    "    tickers = ['AAPL', 'NVDA', 'TSLA']  # Now you can just list your tickers here\n",
    "    stock_data_processor.load_and_combine_sentiment_data(tickers)\n",
    "    stock_data_processor.merge_dataframes()\n",
    "    directory_path = 'result'\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(directory_path)\n",
    "    new_data_for_prediction = stock_data_processor.combined_df.dropna(subset=['mentions'])\n",
    "    cols = list(new_data_for_prediction.columns)\n",
    "    cols.insert(len(cols), cols.pop(cols.index('daily_weighted_avg')))\n",
    "    new_data_for_prediction = new_data_for_prediction.loc[:, cols]\n",
    "    new_data_for_prediction.to_csv('./data_for_prediction/new_data_for_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File new_data_for_prediction.csv uploaded to data_for_prediction/new_data_for_prediction.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "def upload_folder_to_s3(bucket_name, folder_path, s3_folder):\n",
    "    \"\"\"\n",
    "    Upload a folder to an S3 bucket.\n",
    "\n",
    "    :param bucket_name: Name of the S3 bucket.\n",
    "    :param folder_path: Local path to the folder to upload.\n",
    "    :param s3_folder: S3 folder path where files will be uploaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        for subdir, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(subdir, file)\n",
    "                with open(full_path, 'rb') as data:\n",
    "                    file_path_on_s3 = os.path.join(s3_folder, os.path.relpath(full_path, folder_path))\n",
    "                    s3.upload_fileobj(data, bucket_name, file_path_on_s3)\n",
    "                    print(f\"File {file} uploaded to {file_path_on_s3}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "bucket_name = '733-project-new-data'\n",
    "local_folder_path = './data_for_prediction/'\n",
    "s3_folder_path = 'data_for_prediction'\n",
    "\n",
    "upload_folder_to_s3(bucket_name, local_folder_path, s3_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
