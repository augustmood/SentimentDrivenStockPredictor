{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import expr, concat\n",
    "# import findspark\n",
    "# import logging\n",
    "# import time\n",
    "\n",
    "# findspark.init()\n",
    "\n",
    "# # Setup basic configuration for logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# def log_time_taken(start, operation):\n",
    "#     end = time.time()\n",
    "#     logger.info(f\"{operation} completed in {end - start:.2f} seconds\")\n",
    "\n",
    "# # Start timing and log the initialization of the Spark session\n",
    "# logger.info(\"Initializing Spark session with optimized memory settings\")\n",
    "# start_time = time.time()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Reddit Comment Context Builder\") \\\n",
    "#     .master(\"local[*]\")  \\\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  \\\n",
    "#     .config(\"spark.driver.memory\", \"32g\")  \\\n",
    "#     .config(\"spark.executor.memoryOverhead\", \"4096\") \\\n",
    "#     .config(\"spark.driver.memoryOverhead\", \"2048\")  \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "#     .config(\"spark.driver.extraClassPath\", \"/Volumes/LaCie/wsb_archive/postgresql-42.7.3.jar\") \\\n",
    "#     .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "#     .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "#     .getOrCreate()\n",
    "# log_time_taken(start_time, \"SparkSession initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another set of spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 12:05:24,371 - INFO - Initializing Spark session with optimized memory settings\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/04 12:05:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 12:05:26,676 - INFO - SparkSession initialization completed in 2.30 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, concat\n",
    "import findspark\n",
    "import logging\n",
    "import time\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_time_taken(start, operation):\n",
    "    end = time.time()\n",
    "    logger.info(f\"{operation} completed in {end - start:.2f} seconds\")\n",
    "\n",
    "# Start timing and log the initialization of the Spark session\n",
    "logger.info(\"Initializing Spark session with optimized memory settings\")\n",
    "start_time = time.time()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comment Context Builder\") \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .config(\"spark.executor.memory\", \"16g\")  \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")  \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4096\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"2048\")  \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/Volumes/LaCie/wsb_archive/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "log_time_taken(start_time, \"SparkSession initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_comments_with_context = spark.read.parquet(\"./wsb_comments_with_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 12:05:29,402 - INFO - Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-04-04 12:05:29,403 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import re\n",
    "\n",
    "class CompanyNameSimplifier:\n",
    "    def __init__(self):\n",
    "        self.suffixes = [\n",
    "            'Inc.', 'Inc', 'Corporation', 'Corp.', 'Corp', 'Company', 'Co.', 'Co', \n",
    "            'Limited', 'Ltd.', 'Ltd', ' PLC', ' NV', ' SA', ' AG', ' LLC', ' L.P.', ' LP'\n",
    "        ]\n",
    "        # Adjusted to remove web domains in any part of the name before comma, period, or space\n",
    "        self.web_domains_regex = r'\\.com|\\.org|\\.net|\\.io|\\.co|\\.ai'\n",
    "\n",
    "    def simplify_company_name(self, name):\n",
    "        # Remove web domain suffixes using regular expression first\n",
    "        name = re.sub(self.web_domains_regex, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove any company suffix from the list\n",
    "        for suffix in self.suffixes:\n",
    "            if name.endswith(suffix):\n",
    "                name = name.replace(suffix, '')\n",
    "                break\n",
    "        \n",
    "        # Additional cleanup: remove anything after a comma or dash\n",
    "        name = re.split(',| -', name)[0]\n",
    "\n",
    "        # Strip leading and trailing whitespace\n",
    "        name = name.strip()\n",
    "\n",
    "        return name\n",
    "\n",
    "    def get_simplified_company_name(self, ticker):\n",
    "        # Fetch the company info using yfinance\n",
    "        company = yf.Ticker(ticker)\n",
    "        company_info = company.info\n",
    "        \n",
    "        # Extract the long name\n",
    "        full_name = company_info.get('longName', '')\n",
    "        \n",
    "        # Simplify the name\n",
    "        simple_name = self.simplify_company_name(full_name)\n",
    "        \n",
    "        return simple_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_comments_by_ticker(df, ticker):\n",
    "    simplifier = CompanyNameSimplifier()\n",
    "    # Obtain the simplified company name for the given ticker\n",
    "    company_name = simplifier.get_simplified_company_name(ticker)\n",
    "    \n",
    "    # Convert the ticker and company name to lowercase for a case-insensitive search\n",
    "    ticker_lower = ticker.lower()\n",
    "    company_name_lower = company_name.lower()\n",
    "\n",
    "    # Filter the DataFrame for rows where the `comment_context` contains the ticker or the company name\n",
    "    # Uses `lower` function to ensure that the search is case-insensitive\n",
    "    filtered_df = df.filter(\n",
    "        lower(col(\"comment_context\")).contains(ticker_lower) | \n",
    "        lower(col(\"comment_context\")).contains(company_name_lower)\n",
    "    ).select(\"datetime_utc\", \"comment_score\", \"comment_body\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_symbols = ['NVDA', 'TSLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for ticker in ticker_symbols:\n",
    "    comments = filter_comments_by_ticker(wsb_comments_with_context, ticker)\n",
    "    comments.write.parquet(f\"./stock_comments/{ticker}_comments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
