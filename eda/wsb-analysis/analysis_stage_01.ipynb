{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 18:26:55,167 - INFO - Initializing Spark session with optimized memory settings\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/03 18:26:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/03 18:26:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 18:26:57,576 - INFO - SparkSession initialization completed in 2.41 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, concat\n",
    "import findspark\n",
    "import logging\n",
    "import time\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_time_taken(start, operation):\n",
    "    end = time.time()\n",
    "    logger.info(f\"{operation} completed in {end - start:.2f} seconds\")\n",
    "\n",
    "# Start timing and log the initialization of the Spark session\n",
    "logger.info(\"Initializing Spark session with optimized memory settings\")\n",
    "start_time = time.time()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comment Context Builder\") \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .config(\"spark.executor.memory\", \"64g\")  \\\n",
    "    .config(\"spark.driver.memory\", \"32g\")  \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4096\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"2048\")  \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/Volumes/LaCie/wsb_archive/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "log_time_taken(start_time, \"SparkSession initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+-------------------+----------+-------------+----------+-------------+--------------------+--------------+--------------------+-----------+\n",
    "|       datetime_utc|comment_id|submission_id| parent_id|comment_score|        comment_body|curr_parent_id|     comment_context|reached_top|\n",
    "+-------------------+----------+-------------+----------+-------------+--------------------+--------------+--------------------+-----------+\n",
    "|2012-04-11 09:46:43|   c4b0pvu|     t3_s4jw1|  t3_s4jw1|            2|This is a fantast...|      t3_s4jw1|This is a fantast...|       true|\n",
    "|2012-04-11 10:12:16|   c4b127p|     t3_s4jw1|  t3_s4jw1|            1|           [deleted]|      t3_s4jw1|[deleted] |->| Ea...|       true|\n",
    "|2012-04-11 10:39:08|   c4b1fpf|     t3_s4jw1|  t3_s4jw1|            2|     INTC is on 4/17|      t3_s4jw1|INTC is on 4/17 |...|       true|\n",
    "|2012-04-11 11:02:31|   c4b1rmm|     t3_s4jw1|  t3_s4jw1|            1|straddle, call, s...|      t3_s4jw1|straddle, call, s...|       true|\n",
    "|2012-04-11 11:47:11|   c4b2egm|     t3_s4jw1|  t3_s4jw1|            6|GMCR falls, GOOG ...|      t3_s4jw1|GMCR falls, GOOG ...|       true|\n",
    "|2012-04-11 12:44:33|   c4b389t|     t3_s4jw1|  t3_s4jw1|            1|CROX 4/26\\n\\nBZH ...|      t3_s4jw1|CROX 4/26\\n\\nBZH ...|       true|\n",
    "|2012-04-11 13:02:56|   c4b3hli|     t3_s4jw1|  t3_s4jw1|            1|Shorting GOOG and...|      t3_s4jw1|Shorting GOOG and...|       true|\n",
    "|2012-04-11 13:16:44|   c4b3ol0|     t3_s4jw1|  t3_s4jw1|            2|I'm looking at CJ...|      t3_s4jw1|I'm looking at CJ...|       true|\n",
    "|2012-04-11 13:48:27|   c4b44rv|     t3_s4jw1|t1_c4b2egm|            2|GRPN earnings rep...|      t3_s4jw1|GRPN earnings rep...|       true|\n",
    "|2012-04-11 13:54:48|   c4b483q|     t3_s4jw1|t1_c4b44rv|            1|           [deleted]|      t3_s4jw1|[deleted] |->| GR...|       true|\n",
    "|2012-04-11 13:59:34|   c4b4ak3|     t3_s4jw1|t1_c4b3ol0|            2|I'm long CJES. Se...|      t3_s4jw1|I'm long CJES. Se...|       true|\n",
    "|2012-04-11 14:01:27|   c4b4bin|     t3_s4jw1|t1_c4b44rv|            1|Shorting FB for t...|      t3_s4jw1|Shorting FB for t...|       true|\n",
    "|2012-04-11 14:02:43|   c4b4c5j|     t3_s4jw1|  t3_s4jw1|            1|BAC 4/19 betting ...|      t3_s4jw1|BAC 4/19 betting ...|       true|\n",
    "|2012-04-11 14:19:43|   c4b4kkn|     t3_s4jw1|  t3_s4jw1|            1|Shorting AAPL ahe...|      t3_s4jw1|Shorting AAPL ahe...|       true|\n",
    "|2012-04-11 14:55:43|   c4b52dr|     t3_s4jw1|  t3_s4jw1|            2|I believe that no...|      t3_s4jw1|I believe that no...|       true|\n",
    "|2012-04-11 15:29:44|   c4b5ih9|     t3_s4jw1|t1_c4b52dr|            1|could you elabora...|      t3_s4jw1|could you elabora...|       true|\n",
    "|2012-04-11 17:01:19|   c4b6npm|     t3_s4jw1|t1_c4b4c5j|            1|           me too :)|      t3_s4jw1|me too :) |->| BA...|       true|\n",
    "|2012-04-11 17:10:37|   c4b6rk0|     t3_s4jw1|t1_c4b4ak3|            1|Yes, and especial...|      t3_s4jw1|Yes, and especial...|       true|\n",
    "|2012-04-11 20:39:07|   c4b9fod|     t3_s4jw1|  t3_s4jw1|            1|How can I learn a...|      t3_s4jw1|How can I learn a...|       true|\n",
    "|2012-04-12 06:40:24|   c4bdsvy|     t3_s4jw1|t1_c4b9fod|            1|           [deleted]|      t3_s4jw1|[deleted] |->| Ho...|       true|\n",
    "+-------------------+----------+-------------+----------+-------------+--------------------+--------------+--------------------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wsb_comments_with_context = spark.read.parquet(\"./wsb_comments_with_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 18:27:00,491 - INFO - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-04-03 18:27:00,492 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import re\n",
    "\n",
    "class CompanyNameSimplifier:\n",
    "    def __init__(self):\n",
    "        self.suffixes = [\n",
    "            'Inc.', 'Inc', 'Corporation', 'Corp.', 'Corp', 'Company', 'Co.', 'Co', \n",
    "            'Limited', 'Ltd.', 'Ltd', ' PLC', ' NV', ' SA', ' AG', ' LLC', ' L.P.', ' LP'\n",
    "        ]\n",
    "        # Adjusted to remove web domains in any part of the name before comma, period, or space\n",
    "        self.web_domains_regex = r'\\.com|\\.org|\\.net|\\.io|\\.co|\\.ai'\n",
    "\n",
    "    def simplify_company_name(self, name):\n",
    "        # Remove web domain suffixes using regular expression first\n",
    "        name = re.sub(self.web_domains_regex, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove any company suffix from the list\n",
    "        for suffix in self.suffixes:\n",
    "            if name.endswith(suffix):\n",
    "                name = name.replace(suffix, '')\n",
    "                break\n",
    "        \n",
    "        # Additional cleanup: remove anything after a comma or dash\n",
    "        name = re.split(',| -', name)[0]\n",
    "\n",
    "        # Strip leading and trailing whitespace\n",
    "        name = name.strip()\n",
    "\n",
    "        return name\n",
    "\n",
    "    def get_simplified_company_name(self, ticker):\n",
    "        # Fetch the company info using yfinance\n",
    "        company = yf.Ticker(ticker)\n",
    "        company_info = company.info\n",
    "        \n",
    "        # Extract the long name\n",
    "        full_name = company_info.get('longName', '')\n",
    "        \n",
    "        # Simplify the name\n",
    "        simple_name = self.simplify_company_name(full_name)\n",
    "        \n",
    "        return simple_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df_shape(df):\n",
    "    \"\"\"\n",
    "    Calculates the shape of a DataFrame more efficiently by writing to and reading from a temporary file,\n",
    "    and then deletes the temporary file.\n",
    "    \n",
    "    Args:\n",
    "    - df: The Spark DataFrame whose shape is to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple representing the shape (number of rows, number of columns) of the DataFrame.\n",
    "    \"\"\"\n",
    "    # Define path for the temporary file\n",
    "    temp_path = \"./temp\"\n",
    "    \n",
    "    # Write the DataFrame to a temporary location. Parquet is a good choice for efficiency.\n",
    "    df.write.mode(\"overwrite\").parquet(temp_path)\n",
    "    \n",
    "    # Read back the DataFrame, potentially just one column to speed up the count operation if needed\n",
    "    # If the column count is dynamic, consider removing .select()\n",
    "    temp_df = spark.read.parquet(temp_path)\n",
    "    \n",
    "    # Use the count function on the simplified DataFrame\n",
    "    num_rows = temp_df.count()\n",
    "    num_columns = len(df.columns)\n",
    "    \n",
    "    # Cleanup: Delete the temporary directory and its contents\n",
    "    try:\n",
    "        sc = spark.sparkContext\n",
    "        path = sc._jvm.org.apache.hadoop.fs.Path(temp_path)\n",
    "        fs = path.getFileSystem(sc._jvm.org.apache.hadoop.conf.Configuration())\n",
    "        if fs.exists(path):\n",
    "            fs.delete(path, True)  # True for recursive delete\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete temporary files at {temp_path}: {e}\")\n",
    "\n",
    "    # Return the shape\n",
    "    return (num_rows, num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL: Apple\n",
      "NVDA: NVIDIA\n",
      "TSLA: Tesla\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "simplifier = CompanyNameSimplifier()\n",
    "ticker_symbols = ['AAPL', 'NVDA', 'TSLA']\n",
    "for ticker in ticker_symbols:\n",
    "    print(f\"{ticker}: {simplifier.get_simplified_company_name(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "def filter_comments_by_ticker(df, ticker):\n",
    "    simplifier = CompanyNameSimplifier()\n",
    "    # Obtain the simplified company name for the given ticker\n",
    "    company_name = simplifier.get_simplified_company_name(ticker)\n",
    "    \n",
    "    # Convert the ticker and company name to lowercase for a case-insensitive search\n",
    "    ticker_lower = ticker.lower()\n",
    "    company_name_lower = company_name.lower()\n",
    "\n",
    "    # Filter the DataFrame for rows where the `comment_context` contains the ticker or the company name\n",
    "    # Uses `lower` function to ensure that the search is case-insensitive\n",
    "    filtered_df = df.filter(\n",
    "        lower(col(\"comment_context\")).contains(ticker_lower) | \n",
    "        lower(col(\"comment_context\")).contains(company_name_lower)\n",
    "    ).select(\"datetime_utc\", \"comment_score\", \"comment_body\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_comments = filter_comments_by_ticker(wsb_comments_with_context, 'AAPL').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+\n",
      "|       datetime_utc|comment_score|        comment_body|\n",
      "+-------------------+-------------+--------------------+\n",
      "|2012-04-11 09:46:43|            2|This is a fantast...|\n",
      "|2012-04-11 10:12:16|            1|           [deleted]|\n",
      "|2012-04-11 10:39:08|            2|     INTC is on 4/17|\n",
      "|2012-04-11 11:02:31|            1|straddle, call, s...|\n",
      "|2012-04-11 11:47:11|            6|GMCR falls, GOOG ...|\n",
      "|2012-04-11 12:44:33|            1|CROX 4/26\\n\\nBZH ...|\n",
      "|2012-04-11 13:02:56|            1|Shorting GOOG and...|\n",
      "|2012-04-11 13:16:44|            2|I'm looking at CJ...|\n",
      "|2012-04-11 13:48:27|            2|GRPN earnings rep...|\n",
      "|2012-04-11 13:54:48|            1|           [deleted]|\n",
      "|2012-04-11 13:59:34|            2|I'm long CJES. Se...|\n",
      "|2012-04-11 14:01:27|            1|Shorting FB for t...|\n",
      "|2012-04-11 14:02:43|            1|BAC 4/19 betting ...|\n",
      "|2012-04-11 14:19:43|            1|Shorting AAPL ahe...|\n",
      "|2012-04-11 14:55:43|            2|I believe that no...|\n",
      "|2012-04-11 15:29:44|            1|could you elabora...|\n",
      "|2012-04-11 17:01:19|            1|           me too :)|\n",
      "|2012-04-11 17:10:37|            1|Yes, and especial...|\n",
      "|2012-04-11 20:39:07|            1|How can I learn a...|\n",
      "|2012-04-12 06:40:24|            1|           [deleted]|\n",
      "+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "AAPL_comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df_shape(AAPL_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAPL_comments.write.parquet('./stock_comments/AAPL_comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
